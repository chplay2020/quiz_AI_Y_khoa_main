"""
PocketFlow Nodes for Medical Quiz Generation
Implements the workflow nodes for document processing, RAG, and question generation
"""
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
import asyncio
import json

from requests import options
import structlog

# PocketFlow-style base classes (simplified implementation)
from abc import ABC, abstractmethod

from app.core.document_processor import DocumentProcessor, ProcessedDocument
from app.core.rag_engine import RAGEngine, RetrievedContext, get_rag_engine
from app.core.llm_provider import LLMProvider, get_llm_provider
from app.config import settings

logger = structlog.get_logger()


# ============================================
# Base Node Classes (PocketFlow-style)
# ============================================

class BaseNode(ABC):
    """Base class for all PocketFlow nodes"""
    
    def __init__(self, name: str = None):
        self.name = name or self.__class__.__name__
        self.successors: Dict[str, 'BaseNode'] = {}
    
    def add_successor(self, node: 'BaseNode', condition: str = "default") -> 'BaseNode':
        """Add a successor node with an optional condition"""
        self.successors[condition] = node
        return node
    
    @abstractmethod
    async def prep(self, shared_state: Dict[str, Any]) -> Any:
        """Prepare data for execution"""
        pass
    
    @abstractmethod
    async def exec(self, prep_result: Any) -> Any:
        """Execute the node's main logic"""
        pass
    
    @abstractmethod
    async def post(self, shared_state: Dict[str, Any], prep_result: Any, exec_result: Any) -> str:
        """Post-process and determine next node"""
        pass
    
    async def run(self, shared_state: Dict[str, Any]) -> str:
        """Run the complete node lifecycle"""
        logger.info(f"Running node: {self.name}")
        
        prep_result = await self.prep(shared_state)
        exec_result = await self.exec(prep_result)
        next_action = await self.post(shared_state, prep_result, exec_result)
        
        return next_action


class BatchNode(BaseNode):
    """Node that processes items in batches"""
    
    @abstractmethod
    async def prep(self, shared_state: Dict[str, Any]) -> List[Any]:
        """Return a list of items to process"""
        pass
    
    @abstractmethod
    async def exec(self, item: Any) -> Any:
        """Process a single item"""
        pass
    
    async def run(self, shared_state: Dict[str, Any]) -> str:
        """Run batch processing"""
        logger.info(f"Running batch node: {self.name}")
        
        items = await self.prep(shared_state)
        results = []
        
        for item in items:
            result = await self.exec(item)
            results.append(result)
        
        next_action = await self.post(shared_state, items, results)
        return next_action


class Flow:
    """Flow orchestrator that runs a sequence of nodes"""
    
    def __init__(self, start_node: BaseNode):
        self.start_node = start_node
    
    async def run(self, initial_state: Dict[str, Any] = None) -> Dict[str, Any]:
        """Run the flow from start to end"""
        shared_state = initial_state or {}
        current_node = self.start_node
        
        while current_node:
            next_action = await current_node.run(shared_state)
            
            if next_action in current_node.successors:
                current_node = current_node.successors[next_action]
            else:
                current_node = None
        
        return shared_state


# ============================================
# Document Processing Nodes
# ============================================

class DocumentIngestionNode(BaseNode):
    """Node for ingesting and processing documents"""
    
    def __init__(self):
        super().__init__("DocumentIngestion")
        self.processor = DocumentProcessor(
            chunk_size=settings.CHUNK_SIZE,
            chunk_overlap=settings.CHUNK_OVERLAP
        )
    
    async def prep(self, shared_state: Dict[str, Any]) -> Dict[str, Any]:
        """Extract file info from shared state"""
        return {
            'file_path': shared_state.get('file_path'),
            'document_id': shared_state.get('document_id'),
            'metadata': shared_state.get('metadata', {})
        }
    
    async def exec(self, prep_result: Dict[str, Any]) -> ProcessedDocument:
        """Process the document"""
        processed = await self.processor.process_file(
            file_path=prep_result['file_path'],
            document_id=prep_result['document_id'],
            metadata=prep_result['metadata']
        )
        return processed
    
    async def post(self, shared_state: Dict[str, Any], prep_result: Any, exec_result: ProcessedDocument) -> str:
        """Store processed document in shared state"""
        shared_state['processed_document'] = exec_result
        shared_state['num_chunks'] = len(exec_result.chunks)
        
        logger.info(
            "Document processed",
            document_id=exec_result.document_id,
            chunks=len(exec_result.chunks)
        )
        
        return "default"


class EmbeddingNode(BaseNode):
    """Node for generating embeddings and storing in vector DB"""
    
    def __init__(self):
        super().__init__("Embedding")
        self.rag_engine = None
    
    async def prep(self, shared_state: Dict[str, Any]) -> ProcessedDocument:
        """Get processed document from shared state"""
        if self.rag_engine is None:
            self.rag_engine = get_rag_engine()
        return shared_state.get('processed_document')
    
    async def exec(self, processed_doc: ProcessedDocument) -> int:
        """Generate embeddings and store in vector DB"""
        num_stored = await self.rag_engine.add_document(processed_doc)
        return num_stored
    
    async def post(self, shared_state: Dict[str, Any], prep_result: Any, exec_result: int) -> str:
        """Update shared state with embedding info"""
        shared_state['chunks_embedded'] = exec_result
        
        logger.info("Embeddings stored", count=exec_result)
        
        return "default"


# ============================================
# Retrieval Nodes
# ============================================

class ContextRetrievalNode(BaseNode):
    """Node for retrieving relevant contexts for question generation"""
    
    def __init__(self):
        super().__init__("ContextRetrieval")
        self.rag_engine = None
    
    async def prep(self, shared_state: Dict[str, Any]) -> Dict[str, Any]:
        """Extract retrieval parameters"""
        if self.rag_engine is None:
            self.rag_engine = get_rag_engine()
        
        return {
            'document_ids': shared_state.get('document_ids', []),
            'topics': shared_state.get('topics', []),
            'num_contexts': shared_state.get('num_questions', 10) * 2,
            'focus_areas': shared_state.get('focus_areas', [])
        }
    
    async def exec(self, prep_result: Dict[str, Any]) -> List[RetrievedContext]:
        """Retrieve relevant contexts"""
        contexts = await self.rag_engine.search_for_question_generation(
            topics=prep_result['topics'] or prep_result['focus_areas'],
            document_ids=prep_result['document_ids'],
            num_contexts=prep_result['num_contexts']
        )
        return contexts
    
    async def post(self, shared_state: Dict[str, Any], prep_result: Any, exec_result: List[RetrievedContext]) -> str:
        """Store retrieved contexts (and optionally filter by output language)."""
        # language = shared_state.get('language', 'vi')
        # contexts = exec_result or []

        # # FIX 1: l·ªçc chunk nhi·ªÖu/watermark v√† (tu·ª≥ ch·ªçn) gi·ªØ ƒë√∫ng ng√¥n ng·ªØ y√™u c·∫ßu
        # if language == 'vi':
        #     import re

        #     def _looks_vietnamese(s: str) -> bool:
        #         if not s:
        #             return False
        #         if re.search(r"[√Ä-·ªπ]", s):
        #             return True
        #         s2 = f" {s.lower()} "
        #         hits = sum(w in s2 for w in [" v√† ", " c·ªßa ", " kh√¥ng ", " ƒë∆∞·ª£c ", " b·ªánh ", " ƒëi·ªÅu tr·ªã ", " c·∫•p c·ª©u "])
        #         return hits >= 2

        #     filtered: List[RetrievedContext] = []
        #     for c in contexts:
        #         txt = getattr(c, 'content', '') or ''
        #         txt_wo_mark = re.sub(r"(?i)\byhocdata\.com\b", " ", txt).strip()
        #         if len(txt_wo_mark) < 80:
        #             continue
        #         if _looks_vietnamese(txt_wo_mark):
        #             filtered.append(c)

        #     if filtered:
        #         contexts = filtered

        # shared_state['retrieved_contexts'] = contexts
        # logger.info("Contexts retrieved", count=len(contexts))

        # if len(contexts) == 0:
        #     return "no_contexts"

        # return "default"

        shared_state["retrieved_contexts"] = exec_result

        logger.warning(
            "CONTEXT RETRIEVAL RESULT",
            count=len(exec_result),
            sample=exec_result[0].content[:200] if exec_result else "EMPTY"
        )

        if len(exec_result) == 0:
            logger.error("NO CONTEXT RETRIEVED ‚Äì FLOW STUCK")
            return "no_contexts"

        return "default"


class QuestionGenerationNode(BatchNode):
    """Node for generating questions from contexts"""
    
    def __init__(self, llm_provider: Optional[LLMProvider] = None):
        super().__init__("QuestionGeneration")
        self.llm = llm_provider or get_llm_provider()
        self.questions_per_context = 1
    
    async def prep(self, shared_state: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Prepare contexts for question generation"""
        contexts = shared_state.get('retrieved_contexts', [])
        target_questions = int(shared_state.get('num_questions', 10))
        difficulty = shared_state.get('difficulty', 'medium')
        question_types = shared_state.get('question_types', ['single_choice'])
        language = shared_state.get('language', 'vi')
        include_case_based = shared_state.get('include_case_based', False)
        
        # FIX: T√≠nh s·ªë c√¢u h·ªèi th∆∞·ªùng = t·ªïng - s·ªë c√¢u l√¢m s√†ng (n·∫øu c√≥)
        if include_case_based:
            num_case_questions = max(2, target_questions // 3)  # 30% l√† c√¢u l√¢m s√†ng
            regular_target = target_questions - num_case_questions
        else:
            num_case_questions = 0
            regular_target = target_questions
        
        # L∆∞u th√¥ng tin ƒë·ªÉ c√°c node kh√°c s·ª≠ d·ª•ng
        shared_state['regular_target'] = regular_target
        shared_state['case_target'] = num_case_questions
        shared_state['question_target'] = target_questions
        
        # T·∫°o buffer cho c√¢u h·ªèi th∆∞·ªùng (ƒë·ªÅ ph√≤ng JSON l·ªói / b·ªã filter)
        buffer_target = regular_target + max(1, regular_target // 4)

        if not contexts:
            return []

        # D√πng t·ªëi ƒëa `buffer_target` contexts
        num_contexts_to_use = min(len(contexts), buffer_target)
        selected_contexts = contexts[:num_contexts_to_use]

        logger.info(
            "Question generation strategy",
            total_target=target_questions,
            regular_target=regular_target,
            case_target=num_case_questions,
            buffer_target=buffer_target,
            include_case_based=include_case_based,
            total_contexts_available=len(contexts),
            num_contexts_to_use=num_contexts_to_use,
            language=language
        )

        # ===== G·ªòP CONTEXT TH√ÄNH 1 PROMPT =====
        combined_context = "\n\n".join(
            f"[CONTEXT {i+1}]\n{ctx.content[:500]}"
            for i, ctx in enumerate(selected_contexts)
        )

        items = [{
            'context': combined_context,
            'original_contexts': selected_contexts,
            'difficulty': difficulty,
            'question_types': question_types,
            'language': language,
            'num_questions': buffer_target,
        }]

        shared_state['unused_contexts'] = contexts[num_contexts_to_use:]
        shared_state['question_buffer_target'] = buffer_target

        return items

    async def exec(self, item: Dict[str, Any]) -> Dict[str, Any]:
        """Generate questions for a single context"""
        context = item['context']
        difficulty = item['difficulty']
        language = item['language']
        
        # Build the prompt
        prompt = self._build_question_prompt(
            context=context,  # context is already a string after FIX 4 combined contexts
            difficulty=difficulty,
            language=language,
            num_questions=item.get('num_questions', 1)
        )
        
        system_prompt = self._get_system_prompt(language)
        
        try:
            await asyncio.sleep(20)  # üî• FIX RATE LIMIT
            result = await self.llm.generate_structured(
                prompt=prompt,
                system_prompt=system_prompt,
                temperature=0.4
            )
            
            # Handle case where result is a string instead of dict
            if isinstance(result, str):
                import json
                try:
                    result = json.loads(result)
                except json.JSONDecodeError:
                    logger.error("Failed to parse LLM response as JSON", response=result[:200])
                    return {'questions': [], 'error': 'Invalid JSON response'}
            
            # Ensure result is a dict
            if not isinstance(result, dict):
                return {'questions': [], 'error': 'Invalid response format'}
            
            # Add context reference - distribute document_ids across questions
            original_contexts = item.get('original_contexts', [])
            if 'questions' in result and original_contexts:
                # T·∫°o map document_id -> contexts
                doc_contexts_map = {}
                for ctx in original_contexts:
                    doc_id = getattr(ctx, 'document_id', 'unknown')
                    if doc_id not in doc_contexts_map:
                        doc_contexts_map[doc_id] = []
                    doc_contexts_map[doc_id].append(ctx)
                
                doc_ids = list(doc_contexts_map.keys())
                num_questions = len(result['questions'])
                
                # Ph√¢n b·ªï c√¢u h·ªèi ƒë·ªÅu cho c√°c document
                for i, q in enumerate(result['questions']):
                    # Round-robin assignment ƒë·ªÉ ƒë·∫£m b·∫£o ƒë·ªÅu c√°c document
                    assigned_doc_id = doc_ids[i % len(doc_ids)] if doc_ids else 'unknown'
                    assigned_contexts = doc_contexts_map.get(assigned_doc_id, original_contexts)
                    assigned_ctx = assigned_contexts[0] if assigned_contexts else original_contexts[0]
                    
                    q['source_chunk_id'] = getattr(assigned_ctx, 'chunk_id', 'combined')
                    q['document_id'] = assigned_doc_id
                    q['reference_text'] = getattr(assigned_ctx, 'content', '')[:500]
                    
                logger.info(
                    "Questions assigned to documents",
                    num_questions=num_questions,
                    num_documents=len(doc_ids),
                    doc_ids=doc_ids
                )
            
            return result
            
        except Exception as e:
            logger.error("Question generation failed", error=str(e))
            return {'questions': [], 'error': str(e)}
    
    async def post(self, shared_state: Dict[str, Any], prep_result: Any, exec_result: List[Dict[str, Any]]) -> str:
        """Aggregate generated questions"""
        all_questions = []
        
        logger.info("Aggregating question generation results", 
                   num_results=len(exec_result),
                   num_contexts_processed=len(prep_result) if isinstance(prep_result, list) else 1)
        
        for i, result in enumerate(exec_result):
            questions = result.get('questions', [])
            error = result.get('error')
            
            if error:
                logger.warning(f"Context {i} had error", error=error)
            
            if isinstance(questions, list):
                all_questions.extend(questions)
                logger.info(f"Context {i} generated {len(questions)} questions")
            else:
                logger.error(
                    "Invalid questions format from LLM",
                    context_index=i,
                    questions_type=type(questions).__name__,
                    questions_value=str(questions)[:200]
                )
        
        regular_target = int(shared_state.get("regular_target", shared_state.get("num_questions", 10)))
        
        # =========================
        # RETRY N·∫æU KH√îNG ƒê·ª¶ C√ÇU H·ªéI
        # =========================
        retry_count = shared_state.get('_retry_count', 0)
        max_retries = 2
        
        if len(all_questions) < regular_target and retry_count < max_retries:
            missing = regular_target - len(all_questions)
            logger.warning(
                "Not enough questions generated, will retry",
                generated=len(all_questions),
                target=regular_target,
                missing=missing,
                retry_count=retry_count
            )
            
            # L∆∞u c√¢u h·ªèi ƒë√£ c√≥ v√† retry
            shared_state['_partial_questions'] = shared_state.get('_partial_questions', []) + all_questions
            shared_state['_retry_count'] = retry_count + 1
            shared_state['num_questions'] = missing + 2  # Th√™m buffer
            shared_state['regular_target'] = missing + 2
            
            # D√πng unused contexts n·∫øu c√≥
            unused = shared_state.get('unused_contexts', [])
            if unused:
                shared_state['retrieved_contexts'] = unused
                shared_state['unused_contexts'] = []
            
            return "retry"  # Signal to retry
        
        # G·ªôp v·ªõi c√¢u h·ªèi t·ª´ retry tr∆∞·ªõc
        partial = shared_state.get('_partial_questions', [])
        if partial:
            all_questions = partial + all_questions
            # Clear partial
            shared_state.pop('_partial_questions', None)
            shared_state.pop('_retry_count', None)
        
        # =========================
        # C·∫ÆT ƒê√öNG S·ªê L∆Ø·ª¢NG C√ÇU H·ªéI TH∆Ø·ªúNG
        # =========================
        question_target = int(shared_state.get("question_target", regular_target))
        include_case_based = shared_state.get('include_case_based', False)
        
        if include_case_based:
            case_target = shared_state.get('case_target', 0)
            final_regular_target = question_target - case_target
        else:
            final_regular_target = question_target

        if len(all_questions) > final_regular_target:
            all_questions = all_questions[:final_regular_target]

        shared_state["generated_questions"] = all_questions
        shared_state["generated_count"] = len(all_questions)
        shared_state["missing_questions"] = max(0, final_regular_target - len(all_questions))

        logger.info(
            "Regular questions finalized",
            regular_target=final_regular_target,
            question_target=question_target,
            generated=len(all_questions),
            missing=shared_state["missing_questions"]
        )

        if not all_questions:
            logger.warning("No valid questions generated from any context")
        
        logger.info("Questions generated", count=len(all_questions))
        
        return "default"
    
    def _get_system_prompt(self, language: str) -> str:
        """Get system prompt - always Vietnamese"""
        return """
B·∫°n l√† chuy√™n gia y khoa.

‚ö†Ô∏è QUY ƒê·ªäNH B·∫ÆT BU·ªòC:
- B·∫ÆT BU·ªòC s·ª≠ d·ª•ng TI·∫æNG VI·ªÜT
- TUY·ªÜT ƒê·ªêI KH√îNG d√πng ti·∫øng Anh
- KH√îNG d·ªãch sang ti·∫øng Anh
- KH√îNG gi·∫£i th√≠ch b·∫±ng ti·∫øng Anh
- M·ªçi c√¢u h·ªèi, ƒë√°p √°n, gi·∫£i th√≠ch, ch·ªß ƒë·ªÅ, keywords ƒë·ªÅu ph·∫£i b·∫±ng ti·∫øng Vi·ªát

N·∫øu vi ph·∫°m ‚Üí c√¢u tr·∫£ l·ªùi b·ªã coi l√† SAI.

Ch·ªâ tr·∫£ v·ªÅ JSON thu·∫ßn, kh√¥ng markdown, kh√¥ng ```.

"""
    
    def _build_question_prompt(
        self,
        context: str,
        difficulty: str,
        language: str,
        num_questions: int = 1
    ) -> str:
        """Build the question generation prompt - always Vietnamese"""
        difficulty_instruction = {
            'easy': 'C√¢u h·ªèi ƒë∆°n gi·∫£n, ki·ªÉm tra ki·∫øn th·ª©c c∆° b·∫£n',
            'medium': 'C√¢u h·ªèi trung b√¨nh, y√™u c·∫ßu hi·ªÉu v√† √°p d·ª•ng ki·∫øn th·ª©c',
            'hard': 'C√¢u h·ªèi kh√≥, y√™u c·∫ßu ph√¢n t√≠ch v√† t·ªïng h·ª£p ki·∫øn th·ª©c'
        }
        
        prompt = f"""D·ª±a tr√™n n·ªôi dung y khoa sau ƒë√¢y, h√£y t·∫°o CH√çNH X√ÅC {num_questions} c√¢u h·ªèi tr·∫Øc nghi·ªám.

‚ö†Ô∏è QUAN TR·ªåNG: B·∫ÆT BU·ªòC t·∫°o ƒë·ªß {num_questions} c√¢u h·ªèi. Kh√¥ng ƒë∆∞·ª£c t·∫°o √≠t h∆°n!

N·ªòI DUNG:
{context}

Y√äU C·∫¶U:
- S·ªë l∆∞·ª£ng c√¢u h·ªèi: CH√çNH X√ÅC {num_questions} c√¢u (kh√¥ng ƒë∆∞·ª£c thi·∫øu!)
- ƒê·ªô kh√≥: {difficulty_instruction.get(difficulty, difficulty_instruction['medium'])}
- M·ªói c√¢u h·ªèi c√≥ 4 l·ª±a ch·ªçn (A, B, C, D)
- Ch·ªâ c√≥ 1 ƒë√°p √°n ƒë√∫ng
- Cung c·∫•p gi·∫£i th√≠ch chi ti·∫øt cho ƒë√°p √°n ƒë√∫ng
- T·∫•t c·∫£ n·ªôi dung ph·∫£i b·∫±ng TI·∫æNG VI·ªÜT
- Kh√¥ng d√πng markdown, kh√¥ng b·ªçc b·∫±ng ```
- M·ªói c√¢u h·ªèi ph·∫£i kh√°c nhau, kh√¥ng tr√πng l·∫∑p n·ªôi dung

Tr·∫£ v·ªÅ JSON theo format sau:
{{
    "questions": [
        {{
            "question_text": "N·ªôi dung c√¢u h·ªèi",
            "question_type": "single_choice",
            "difficulty": "{difficulty}",
            "options": [
                {{"id": "A", "text": "L·ª±a ch·ªçn A", "is_correct": false}},
                {{"id": "B", "text": "L·ª±a ch·ªçn B", "is_correct": true}},
                {{"id": "C", "text": "L·ª±a ch·ªçn C", "is_correct": false}},
                {{"id": "D", "text": "L·ª±a ch·ªçn D", "is_correct": false}}
            ],
            "correct_answer": "B",
            "explanation": "Gi·∫£i th√≠ch chi ti·∫øt t·∫°i sao B l√† ƒë√°p √°n ƒë√∫ng...",
            "topic": "Ch·ªß ƒë·ªÅ c·ªßa c√¢u h·ªèi",
            "keywords": ["t·ª´ kh√≥a 1", "t·ª´ kh√≥a 2"]
        }}
    ]
}}

Nh·∫Øc l·∫°i: Ph·∫£i t·∫°o CH√çNH X√ÅC {num_questions} c√¢u h·ªèi trong m·∫£ng "questions"."""
        
        return prompt


class CaseBasedQuestionNode(BaseNode):
    """Node for generating case-based clinical questions"""
    
    def __init__(self, llm_provider: Optional[LLMProvider] = None):
        super().__init__("CaseBasedQuestion")
        self.llm = llm_provider or get_llm_provider()
    
    async def prep(self, shared_state: Dict[str, Any]) -> Dict[str, Any]:
        """Prepare for case-based question generation"""
        # L·∫•y s·ªë c√¢u l√¢m s√†ng ƒë√£ t√≠nh t·ª´ QuestionGenerationNode
        num_case_questions = shared_state.get('case_target', 0)
        
        # Fallback n·∫øu kh√¥ng c√≥
        if num_case_questions == 0:
            target_questions = shared_state.get('question_target', shared_state.get('num_questions', 10))
            num_case_questions = max(2, target_questions // 3)
        
        logger.info(
            "Preparing case-based questions",
            num_case_questions=num_case_questions,
            total_target=shared_state.get('question_target')
        )
        
        return {
            'contexts': shared_state.get('retrieved_contexts', []),
            'language': shared_state.get('language', 'vi'),
            'num_cases': num_case_questions,
            'difficulty': shared_state.get('difficulty', 'medium')
        }
    
    async def exec(self, prep_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Generate case-based questions"""
        contexts = prep_result['contexts']
        language = prep_result['language']
        num_cases = prep_result['num_cases']
        difficulty = prep_result.get('difficulty', 'medium')
        
        if not contexts:
            logger.warning("No contexts available for case-based questions")
            return []
        
        # Combine multiple contexts for richer case scenarios
        combined_context = "\n\n".join([ctx.content[:600] for ctx in contexts[:8]])
        
        prompt = self._build_case_prompt(combined_context, language, num_cases, difficulty)
        
        try:
            await asyncio.sleep(20)  # Rate limit delay
            result = await self.llm.generate_structured(
                prompt=prompt,
                system_prompt=self._get_system_prompt(language),
                temperature=0.5
            )
            
            logger.info("Case-based LLM response received", result_type=type(result).__name__)
            
            # Handle case where result is a string instead of dict
            if isinstance(result, str):
                import json
                try:
                    result = json.loads(result)
                except json.JSONDecodeError:
                    logger.error("Failed to parse case-based response as JSON", response=result[:300])
                    return []
            
            if not isinstance(result, dict):
                logger.error("Invalid result type for case-based", result_type=type(result).__name__)
                return []
            
            cases = result.get('cases', [])
            logger.info("Case-based questions parsed", num_cases=len(cases))
            return cases
            
        except Exception as e:
            logger.error("Case-based question generation failed", error=str(e))
            return []
    
    async def post(self, shared_state: Dict[str, Any], prep_result: Any, exec_result: List[Dict[str, Any]]) -> str:
        """Store case-based questions with scenario merged into question_text"""
        existing_questions = shared_state.get('generated_questions', [])
        contexts = prep_result.get('contexts', [])
        case_questions_added = 0
        case_target = shared_state.get('case_target', prep_result.get('num_cases', 2))
        
        # Convert cases to question format
        for case in exec_result:
            scenario = case.get('scenario', '')
            
            if 'questions' in case:
                for q in case['questions']:
                    # ===== FIX: GH√âP SCENARIO V√ÄO QUESTION_TEXT =====
                    original_question = q.get('question_text', '')
                    if scenario:
                        # Gh√©p t√¨nh hu·ªëng l√¢m s√†ng v√†o ƒë·∫ßu c√¢u h·ªèi
                        q['question_text'] = f"üìã T√åNH HU·ªêNG L√ÇM S√ÄNG:\n{scenario}\n\n‚ùì C√ÇU H·ªéI:\n{original_question}"
                    
                    q['question_type'] = 'case_based'
                    q['case_scenario'] = scenario  # Gi·ªØ l·∫°i scenario ri√™ng ƒë·ªÉ reference
                    q['is_clinical'] = True
                    
                    # Th√™m metadata
                    if contexts:
                        q['source_chunk_id'] = getattr(contexts[0], 'chunk_id', 'case_based')
                        q['document_id'] = getattr(contexts[0], 'document_id', 'unknown')
                    
                    # ƒê·∫£m b·∫£o c√≥ ƒë·ªß c√°c tr∆∞·ªùng c·∫ßn thi·∫øt
                    if 'difficulty' not in q:
                        q['difficulty'] = prep_result.get('difficulty', 'medium')
                    if 'topic' not in q:
                        q['topic'] = 'T√¨nh hu·ªëng l√¢m s√†ng'
                    if 'keywords' not in q:
                        q['keywords'] = ['l√¢m s√†ng', 't√¨nh hu·ªëng', 'b·ªánh nh√¢n']
                    
                    existing_questions.append(q)
                    case_questions_added += 1
                    
                    # D·ª´ng n·∫øu ƒë√£ ƒë·ªß s·ªë c√¢u l√¢m s√†ng c·∫ßn thi·∫øt
                    if case_questions_added >= case_target:
                        break
            
            if case_questions_added >= case_target:
                break
        
        shared_state['generated_questions'] = existing_questions
        shared_state['case_questions_count'] = case_questions_added
        
        # Log t·ªïng k·∫øt
        total_questions = len(existing_questions)
        question_target = shared_state.get('question_target', total_questions)
        
        logger.info(
            "Case-based questions added - FINAL COUNT",
            case_questions_added=case_questions_added,
            case_target=case_target,
            total_questions=total_questions,
            question_target=question_target,
            match=(total_questions == question_target)
        )
        
        return "default"
    
    def _get_system_prompt(self, language: str) -> str:
        return """B·∫°n l√† m·ªôt b√°c sƒ© l√¢m s√†ng gi√†u kinh nghi·ªám. H√£y t·∫°o c√°c t√¨nh hu·ªëng l√¢m s√†ng th·ª±c t·∫ø 
v·ªõi c√°c c√¢u h·ªèi tr·∫Øc nghi·ªám li√™n quan. C√°c t√¨nh hu·ªëng ph·∫£i gi·ªëng nh∆∞ g·∫∑p trong th·ª±c h√†nh l√¢m s√†ng.

‚ö†Ô∏è QUY ƒê·ªäNH B·∫ÆT BU·ªòC:
- B·∫ÆT BU·ªòC s·ª≠ d·ª•ng TI·∫æNG VI·ªÜT cho t·∫•t c·∫£ n·ªôi dung
- KH√îNG d√πng ti·∫øng Anh
- M·ªói t√¨nh hu·ªëng ph·∫£i c√≥ ƒë·∫ßy ƒë·ªß: tu·ªïi, gi·ªõi t√≠nh, tri·ªáu ch·ª©ng, ti·ªÅn s·ª≠
- C√¢u h·ªèi ph·∫£i li√™n quan tr·ª±c ti·∫øp ƒë·∫øn t√¨nh hu·ªëng

Ch·ªâ tr·∫£ v·ªÅ JSON thu·∫ßn, kh√¥ng markdown, kh√¥ng ```."""
    
    def _build_case_prompt(self, context: str, language: str, num_cases: int, difficulty: str) -> str:
        difficulty_desc = {
            'easy': 'ƒë∆°n gi·∫£n, tri·ªáu ch·ª©ng ƒëi·ªÉn h√¨nh',
            'medium': 'trung b√¨nh, c·∫ßn ph√¢n t√≠ch',
            'hard': 'ph·ª©c t·∫°p, nhi·ªÅu y·∫øu t·ªë g√¢y nhi·ªÖu'
        }
        
        return f"""D·ª±a tr√™n ki·∫øn th·ª©c y khoa sau, t·∫°o CH√çNH X√ÅC {num_cases} t√¨nh hu·ªëng l√¢m s√†ng v·ªõi c√¢u h·ªèi.

KI·∫æN TH·ª®C THAM KH·∫¢O:
{context}

Y√äU C·∫¶U:
- T·∫°o ƒë√∫ng {num_cases} t√¨nh hu·ªëng l√¢m s√†ng kh√°c nhau
- ƒê·ªô kh√≥: {difficulty_desc.get(difficulty, 'trung b√¨nh')}
- M·ªói t√¨nh hu·ªëng c√≥ 1-2 c√¢u h·ªèi tr·∫Øc nghi·ªám
- T√¨nh hu·ªëng ph·∫£i th·ª±c t·∫ø, chi ti·∫øt (tu·ªïi, gi·ªõi, tri·ªáu ch·ª©ng c·ª• th·ªÉ)
- T·∫§T C·∫¢ b·∫±ng TI·∫æNG VI·ªÜT

Tr·∫£ v·ªÅ JSON theo format sau (KH√îNG d√πng markdown):
{{
    "cases": [
        {{
            "scenario": "B·ªánh nh√¢n nam 45 tu·ªïi, v√†o vi·ªán v√¨ ƒëau ng·ª±c tr√°i 2 gi·ªù...",
            "questions": [
                {{
                    "question_text": "Ch·∫©n ƒëo√°n ph√π h·ª£p nh·∫•t v·ªõi b·ªánh nh√¢n n√†y l√† g√¨?",
                    "options": [
                        {{"id": "A", "text": "Nh·ªìi m√°u c∆° tim c·∫•p", "is_correct": true}},
                        {{"id": "B", "text": "Vi√™m m√†ng ngo√†i tim", "is_correct": false}},
                        {{"id": "C", "text": "Thuy√™n t·∫Øc ph·ªïi", "is_correct": false}},
                        {{"id": "D", "text": "Vi√™m ph·ªïi", "is_correct": false}}
                    ],
                    "correct_answer": "A",
                    "explanation": "Gi·∫£i th√≠ch chi ti·∫øt t·∫°i sao A l√† ƒë√°p √°n ƒë√∫ng..."
                }}
            ]
        }}
    ]
}}"""


class QuestionValidationNode(BaseNode):
    """Node for validating generated questions"""

    def __init__(self):
        super().__init__("QuestionValidation")

    async def prep(self, shared_state: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Get questions to validate"""
        return shared_state.get('generated_questions', [])

    async def exec(self, questions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Validate questions"""
        validated = []

        for q in questions:
            if not isinstance(q, dict):
                logger.error(
                    "Invalid question item",
                    type=type(q).__name__,
                    value=str(q)[:100]
                )
                continue

            if self._is_valid_question(q):
                validated.append(q)
            else:
                logger.warning(
                    "Invalid question filtered",
                    question=q.get('question_text', '')[:50]
                )

        return validated

    async def post(
        self,
        shared_state: Dict[str, Any],
        prep_result: Any,
        exec_result: List[Dict[str, Any]]
    ) -> str:
        """Store validated questions"""
        shared_state['validated_questions'] = exec_result
        shared_state['validation_stats'] = {
            'total': len(prep_result),
            'valid': len(exec_result),
            'filtered': len(prep_result) - len(exec_result)
        }
        return "default"

    def _is_valid_question(self, question: Dict[str, Any]) -> bool:
        """Check if a question is valid"""

        # Required fields
        required = ['question_text', 'options', 'correct_answer']
        if not all(key in question for key in required):
            return False

        # Question text checks
        question_text = question.get('question_text')
        if not isinstance(question_text, str) or len(question_text.strip()) < 10:
            return False

        # Options checks
        options = question.get('options')
        if not isinstance(options, list) or len(options) < 2:
            return False

        # Validate option structure (FIX CH√çNH)
        option_ids = []
        for opt in options:
            if not isinstance(opt, dict):
                return False

            opt_id = opt.get('id')
            if not isinstance(opt_id, str) or not opt_id:
                return False

            option_ids.append(opt_id)

        # Check that correct answer exists in options
        correct = question.get('correct_answer')
        if not isinstance(correct, str) or correct not in option_ids:
            return False

        # Check that exactly one option is marked correct
        correct_count = sum(1 for opt in options if opt.get('is_correct', False))

        if correct_count != 1:
            # Auto-fix: normalize is_correct flags
            for opt in options:
                opt['is_correct'] = (opt.get('id') == correct)

        return True



class AIDoubleCheckNode(BaseNode):
    """
    Node for AI-powered double-checking of generated questions.
    Uses LLM to verify medical accuracy, clarity, and educational value.
    """
    
    def __init__(self, llm_provider: Optional[LLMProvider] = None):
        super().__init__("AIDoubleCheck")
        self.llm = llm_provider or LLMProvider()
    
    async def prep(self, shared_state: Dict[str, Any]) -> Dict[str, Any]:
        """Get validated questions and context for review"""
        return {
            'questions': shared_state.get('validated_questions', []),
            'context': shared_state.get('retrieved_context', []),
            'topic': shared_state.get('topic', ''),
            'enable_double_check': shared_state.get('enable_double_check', True)
        }
    
    async def exec(self, prep_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Use AI to double-check questions for accuracy and quality"""
        questions = prep_data['questions']
        
        if not prep_data.get('enable_double_check', True) or not questions:
            # Return questions as-is with default review
            for q in questions:
                q['ai_review'] = {
                    'status': 'skipped',
                    'accuracy_score': None,
                    'clarity_score': None,
                    'suggestions': [],
                    'reviewed': False
                }
            return questions
        
        logger.info("Starting AI double-check", num_questions=len(questions))
        
        reviewed_questions = []
        
        # Process in batches of 5 for efficiency
        batch_size = 5
        for i in range(0, len(questions), batch_size):
            batch = questions[i:i + batch_size]
            
            prompt = self._build_review_prompt(batch, prep_data.get('context', []))
            
            try:
                response = await self.llm.generate(
                    prompt,
                    max_tokens=2000,
                    temperature=0.3
                )
                
                reviews = self._parse_review_response(response)
                
                for j, q in enumerate(batch):
                    if j < len(reviews):
                        q['ai_review'] = reviews[j]
                    else:
                        q['ai_review'] = self._default_review()
                    reviewed_questions.append(q)
                    
            except Exception as e:
                logger.error("AI review failed", error=str(e))
                for q in batch:
                    q['ai_review'] = self._default_review(error=str(e))
                    reviewed_questions.append(q)
        
        return reviewed_questions
    
    async def post(self, shared_state: Dict[str, Any], prep_result: Any, exec_result: List[Dict[str, Any]]) -> str:
        """Store reviewed questions and generate report"""
        shared_state['reviewed_questions'] = exec_result
        
        # Calculate review statistics
        total = len(exec_result)
        reviewed = sum(1 for q in exec_result if q.get('ai_review', {}).get('reviewed', False))
        
        high_accuracy = sum(1 for q in exec_result 
                          if (q.get('ai_review', {}).get('accuracy_score') or 0) >= 8)
        needs_revision = sum(1 for q in exec_result 
                           if (q.get('ai_review', {}).get('accuracy_score') or 10) < 6)
        
        shared_state['review_stats'] = {
            'total_questions': total,
            'reviewed': reviewed,
            'high_accuracy': high_accuracy,
            'needs_revision': needs_revision,
            'review_rate': reviewed / total if total > 0 else 0
        }
        
        logger.info(
            "AI double-check completed",
            total=total,
            reviewed=reviewed,
            high_accuracy=high_accuracy,
            needs_revision=needs_revision
        )
        
        return "default"
    
    def _build_review_prompt(self, questions: List[Dict], context: List) -> str:
        """Build prompt for AI review"""
        questions_text = ""
        for i, q in enumerate(questions, 1):
            options_text = "\n".join([
                f"   {opt.get('id', chr(64+j))}. {opt.get('text', '')}" 
                for j, opt in enumerate(q.get('options', []), 1)
            ])
            questions_text += f"""
C√¢u h·ªèi {i}:
{q.get('question_text', '')}
ƒê√°p √°n:
{options_text}
ƒê√°p √°n ƒë√∫ng: {q.get('correct_answer', '')}
Gi·∫£i th√≠ch: {q.get('explanation', 'Kh√¥ng c√≥')}
---
"""
        
        return f"""B·∫°n l√† chuy√™n gia gi√°o d·ª•c y khoa, ƒëang ki·ªÉm tra ch·∫•t l∆∞·ª£ng c√¢u h·ªèi tr·∫Øc nghi·ªám.

H√£y ƒë√°nh gi√° t·ª´ng c√¢u h·ªèi d∆∞·ªõi ƒë√¢y v√† cung c·∫•p:
1. ƒêi·ªÉm ch√≠nh x√°c (1-10): Th√¥ng tin y khoa c√≥ ch√≠nh x√°c kh√¥ng?
2. ƒêi·ªÉm r√µ r√†ng (1-10): C√¢u h·ªèi c√≥ r√µ r√†ng, kh√¥ng m∆° h·ªì kh√¥ng?
3. Gi√° tr·ªã gi√°o d·ª•c (1-10): C√¢u h·ªèi c√≥ ki·ªÉm tra ki·∫øn th·ª©c y khoa quan tr·ªçng kh√¥ng?
4. V·∫•n ƒë·ªÅ: Li·ªát k√™ c√°c v·∫•n ƒë·ªÅ ph√°t hi·ªán (ƒë√°p √°n sai, c√¢u h·ªèi m∆° h·ªì, v.v.)
5. G·ª£i √Ω: C√°ch c·∫£i thi·ªán c√¢u h·ªèi
6. K·∫øt lu·∫≠n: ƒê·∫†T, C·∫¶N_S·ª¨a, ho·∫∑c KH√îNG_ƒê·∫†T

C√°c c√¢u h·ªèi c·∫ßn ki·ªÉm tra:
{questions_text}

Tr·∫£ l·ªùi theo ƒë·ªãnh d·∫°ng JSON (B·∫ÆT BU·ªòC d√πng ti·∫øng Vi·ªát cho issues v√† suggestions):
{{
    "reviews": [
        {{
            "question_index": 1,
            "accuracy_score": 8,
            "clarity_score": 9,
            "educational_value": 7,
            "issues": ["M√¥ t·∫£ v·∫•n ƒë·ªÅ b·∫±ng ti·∫øng Vi·ªát"],
            "suggestions": ["G·ª£i √Ω c·∫£i thi·ªán b·∫±ng ti·∫øng Vi·ªát"],
            "verdict": "ƒê·∫†T",
            "corrected_answer": null,
            "corrected_explanation": null
        }}
    ]
}}

L∆∞u √Ω quan tr·ªçng:
- Nghi√™m kh·∫Øc v·ªÅ ƒë·ªô ch√≠nh x√°c y khoa
- ƒê√°nh d·∫•u th√¥ng tin sai c√≥ th·ªÉ g√¢y nguy hi·ªÉm
- N·∫øu ƒë√°p √°n sai, cung c·∫•p ƒë√°p √°n ƒë√∫ng
- Thu·∫≠t ng·ªØ y khoa ti·∫øng Vi·ªát ph·∫£i ch√≠nh x√°c
- T·∫§T C·∫¢ n·ªôi dung issues v√† suggestions ph·∫£i b·∫±ng TI·∫æNG VI·ªÜT"""
    
    def _parse_review_response(self, response: str) -> List[Dict[str, Any]]:
        """Parse AI review response"""
        import json
        import re
        
        reviews = []
        
        # Map Vietnamese verdicts to English status
        verdict_map = {
            'ƒë·∫°t': 'approved',
            'dat': 'approved',
            'approved': 'approved',
            'c·∫ßn_s·ª≠a': 'needs_revision',
            'can_sua': 'needs_revision',
            'c·∫ßn s·ª≠a': 'needs_revision',
            'needs_revision': 'needs_revision',
            'kh√¥ng_ƒë·∫°t': 'reject',
            'khong_dat': 'reject',
            'kh√¥ng ƒë·∫°t': 'reject',
            'reject': 'reject'
        }
        
        try:
            # Extract JSON from response
            json_match = re.search(r'\{[\s\S]*\}', response)
            if json_match:
                data = json.loads(json_match.group())
                
                for review in data.get('reviews', []):
                    verdict = review.get('verdict', 'ƒê·∫†T').lower().strip()
                    status = verdict_map.get(verdict, 'approved')
                    
                    reviews.append({
                        'status': status,
                        'accuracy_score': review.get('accuracy_score'),
                        'clarity_score': review.get('clarity_score'),
                        'educational_value': review.get('educational_value'),
                        'issues': review.get('issues', []),
                        'suggestions': review.get('suggestions', []),
                        'corrected_answer': review.get('corrected_answer'),
                        'corrected_explanation': review.get('corrected_explanation'),
                        'reviewed': True
                    })
        except json.JSONDecodeError as e:
            logger.error("Failed to parse AI review", error=str(e))
        
        return reviews
    
    def _default_review(self, error: str = None) -> Dict[str, Any]:
        """Return default review when AI check is skipped or failed"""
        return {
            'status': 'error' if error else 'skipped',
            'accuracy_score': None,
            'clarity_score': None,
            'educational_value': None,
            'issues': [f"Review failed: {error}"] if error else [],
            'suggestions': [],
            'reviewed': False,
            'error': error
        }


class GeminiThinkingQuestionNode(BaseNode):
    """
    Node for generating questions using Gemini Thinking Mode
    Uploads PDF directly to Gemini without RAG
    """
    
    def __init__(self):
        super().__init__("GeminiThinkingQuestion")
        from app.core.llm_provider import GoogleGeminiThinkingProvider
        self.provider = GoogleGeminiThinkingProvider()
    
    async def prep(self, shared_state: Dict[str, Any]) -> Dict[str, Any]:
        """Prepare data for Gemini Thinking generation"""
        document_ids = shared_state.get('document_ids', [])
        
        # Get document file paths
        from app.api.documents import documents_db
        pdf_files = []
        
        for doc_id in document_ids:
            doc = documents_db.get(doc_id)
            if doc and doc.get('file_type') == 'pdf':
                pdf_files.append({
                    'path': doc.get('file_path'),
                    'id': doc_id,
                    'title': doc.get('title', 'Unknown')
                })
        
        if not pdf_files:
            logger.warning("No PDF files found for Gemini Thinking mode")
            return {'error': 'No PDF files found'}
        
        return {
            'pdf_files': pdf_files,
            'num_questions': shared_state.get('num_questions', 10),
            'difficulty': shared_state.get('difficulty', 'medium'),
            'use_thinking': True,
            'use_google_search': shared_state.get('use_google_search', False),
        }
    
    async def exec(self, prep_result: Dict[str, Any]) -> Dict[str, Any]:
        """Generate questions using Gemini Thinking"""
        if 'error' in prep_result:
            return {'questions': [], 'error': prep_result['error']}
        
        all_questions = []
        pdf_files = prep_result['pdf_files']
        total_questions = prep_result['num_questions']
        questions_per_file = max(1, total_questions // len(pdf_files))
        
        for i, pdf_file in enumerate(pdf_files):
            # Last file gets remaining questions
            if i == len(pdf_files) - 1:
                num_q = total_questions - len(all_questions)
            else:
                num_q = questions_per_file
            
            if num_q <= 0:
                continue
            
            logger.info(
                "Generating questions with Gemini Thinking",
                file=pdf_file['title'],
                num_questions=num_q,
                thinking=prep_result['use_thinking'],
                search=prep_result['use_google_search']
            )
            
            try:
                result = await self.provider.generate_quiz_with_thinking(
                    pdf_path=pdf_file['path'],
                    num_questions=num_q,
                    difficulty=prep_result['difficulty'],
                    use_thinking=prep_result['use_thinking'],
                    use_google_search=prep_result['use_google_search'],
                    temperature=0.3
                )
                
                questions = result.get('questions', [])
                
                # Add document_id to each question
                for q in questions:
                    q['document_id'] = pdf_file['id']
                    if not q.get('source_chunk_id'):
                        q['source_chunk_id'] = 'gemini_thinking'
                
                all_questions.extend(questions)
                
                logger.info(
                    "Generated questions from PDF",
                    file=pdf_file['title'],
                    generated=len(questions),
                    total=len(all_questions)
                )
                
            except Exception as e:
                logger.error(
                    "Gemini Thinking generation failed",
                    file=pdf_file['title'],
                    error=str(e)
                )
                continue
        
        return {
            'questions': all_questions,
            'total': len(all_questions),
            'format': 'quiz'
        }
    
    async def post(self, shared_state: Dict[str, Any], prep_result: Any, exec_result: Dict[str, Any]) -> str:
        """Store generated questions"""
        shared_state['generated_questions'] = exec_result.get('questions', [])
        shared_state['total_generated'] = len(exec_result.get('questions', []))
        
        logger.info(
            "Gemini Thinking generation completed",
            total_questions=shared_state['total_generated']
        )
        
        return "default"


# ============================================
# Flow Builder
# ============================================

def create_document_processing_flow() -> Flow:
    """Create a flow for document ingestion and embedding"""
    ingestion_node = DocumentIngestionNode()
    embedding_node = EmbeddingNode()
    
    ingestion_node.add_successor(embedding_node)
    
    return Flow(ingestion_node)


def create_question_generation_flow(
    llm_provider: Optional[LLMProvider] = None,
    include_case_based: bool = False,
    enable_double_check: bool = False,  # Disabled permanently
    use_gemini_thinking: bool = False
) -> Flow:
    """Create a flow for question generation (AI double-check removed for speed)"""
    
    # If using Gemini Thinking, use direct PDF upload flow
    if use_gemini_thinking:
        gemini_node = GeminiThinkingQuestionNode()
        validation_node = QuestionValidationNode()
        
        gemini_node.add_successor(validation_node)
        
        return Flow(gemini_node)
    
    # Standard RAG-based flow (without double-check)
    retrieval_node = ContextRetrievalNode()
    question_node = QuestionGenerationNode(llm_provider)
    validation_node = QuestionValidationNode()
    
    retrieval_node.add_successor(question_node)
    
    # Add retry loop: question_node -> question_node (on "retry")
    question_node.add_successor(question_node, "retry")
    
    if include_case_based:
        case_node = CaseBasedQuestionNode(llm_provider)
        question_node.add_successor(case_node)
        case_node.add_successor(validation_node)
    else:
        question_node.add_successor(validation_node)
    
    return Flow(retrieval_node)


def create_full_pipeline_flow(
    llm_provider: Optional[LLMProvider] = None,
    enable_double_check: bool = False  # Disabled permanently
) -> Flow:
    """Create a complete pipeline from document to questions (AI double-check removed)"""
    # Document processing
    ingestion_node = DocumentIngestionNode()
    embedding_node = EmbeddingNode()
    
    # Question generation
    retrieval_node = ContextRetrievalNode()
    question_node = QuestionGenerationNode(llm_provider)
    validation_node = QuestionValidationNode()
    
    # Connect the flow (without double-check)
    ingestion_node.add_successor(embedding_node)
    embedding_node.add_successor(retrieval_node)
    retrieval_node.add_successor(question_node)
    question_node.add_successor(validation_node)
    
    return Flow(ingestion_node)